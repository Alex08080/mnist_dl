ğŸ§  Projet MNIST â€” Semaine 1 : Fondations
Ce dÃ©pÃ´t contient les premiÃ¨res Ã©tapes dâ€™un projet de classification dâ€™images avec PyTorch, basÃ© sur le dataset MNIST. Lâ€™objectif est dâ€™explorer les bases du Deep Learning, de structurer un projet de maniÃ¨re professionnelle, et de suivre sa progression au fil des jours.

## âœ… Jour 1 : Setup et installation
  
### ğŸ”§ Installation de Git + crÃ©ation du dÃ©pÃ´t local
### ğŸ Mise en place dâ€™un environnement virtuel (venv) pour Ã©viter les conflits de dÃ©pendances 
### ğŸ“¦ Installation des bibliothÃ¨ques essentielles :
- numpy
-  matplotlib
-  torch
-  torchvision

### ğŸ“š Lecture de la documentation officielle pour comprendre les concepts suivants :
- PyTorch : Tensors, torch.nn, DataLoader
- Torchvision : datasets et transformations
- Matplotlib : visualisation de donnÃ©es
  
## ğŸ—ƒï¸ **Jour 2 : Chargement des donnÃ©es**

### ğŸ“¥ Dataset chargÃ© via torchvision.datasets.MNIST avec download=True
### âš™ï¸ Utilisation du DataLoader pour gÃ©rer les batches et le shuffle
### ğŸ” Test du fonctionnement de next(iter(...)) pour inspecter un batch
### ğŸ“Š Visualisation dâ€™exemples de chiffres avec plt.subplots() :
- gestion des axes
- mise en page avec tight_layout()
### ğŸ§± DÃ©but de structuration du code :
- Encapsulation du chargement dans une classe MNISTDataLoader
- SÃ©paration claire des responsabilitÃ©s
### ğŸ›¡ï¸ CrÃ©ation dâ€™un .gitignore adaptÃ© :
- fichiers temporaires Python
- fichiers liÃ©s Ã  PyTorch
- fichiers systÃ¨me Windows
- modÃ¨les sauvegardÃ©s (*.pth)
        
## ğŸ§  Jour 3 : Premier classifieur convolutionnel
### ğŸ§± Construction dâ€™un modÃ¨le NeuralNet :
- Convolution â†’ ReLU â†’ MaxPooling
- Flatten des tensors
- CrossEntropyLoss (intÃ¨gre log softmax)
### ğŸ“ ComprÃ©hension du rÃ´le de chaque opÃ©ration :
- convolutions pour extraire les features
- Flatten pour passer du 3D au 1D
### ğŸ” Mise en place de la boucle dâ€™entraÃ®nement :
- Parcours par batch
- Calcul de la loss moyenne
- Backpropagation avec loss.backward()
- Mise Ã  jour des poids avec optimizer.step()
### ğŸ§ª Ã‰valuation du modÃ¨le :
- Passage en mode eval()
- DÃ©sactivation du calcul des gradients (with torch.no_grad())
- Calcul de lâ€™accuracy via argmax(dim=1) + comparaison avec labels
### ğŸ“ˆ RÃ©sultats obtenus :
- 99.14 % dâ€™accuracy avec 2 couches convolutionnelles (8 epochs)
- 99.31 % avec 3 couches convolutionnelles (20 epochs)
### ğŸ’¾ Sauvegarde du modÃ¨le avec torch.save()
  
## ğŸ§  Jour 4 : RÃ©gression linÃ©aire, MLP et visualisation

### ğŸ“ˆ RÃ©gression linÃ©aire simple
- ImplÃ©mentation du calcul des logits :  
  \( z = W \times x + b \)  
  oÃ¹ \( W \) est la matrice de poids, \( x \) le vecteur dâ€™entrÃ©e, et \( b \) le biais.  
- InterprÃ©tation de \( z \) comme un vecteur de scores pour chaque classe (logits).  
- Initialisation alÃ©atoire des poids et biais, puis prÃ©diction avant entraÃ®nement.

### ğŸ§® Perceptron multicouche (MLP) basique
- Ajout de la fonction dâ€™activation **softmax** pour convertir les logits en probabilitÃ©s.  
- ImplÃ©mentation de la fonction **cross_entropy** pour mesurer la perte sur la classe cible.  
- Calcul manuel des gradients, avec mise Ã  jour des poids par descente de gradient.  
- Explication dÃ©taillÃ©e du gradient `dz` et du mÃ©canisme `dz[target] -= 1`.

### ğŸ”„ EntraÃ®nement â€œfrom scratchâ€
- Boucle dâ€™entraÃ®nement sur plusieurs epochs avec suivi de la loss.  
- Early stopping manuel lorsque la perte devient suffisamment basse.

### ğŸ§  ComprÃ©hension approfondie de la backpropagation
- RÃ´le du produit extÃ©rieur \( dz[:, np.newaxis] \times x[np.newaxis, :] \) dans le calcul de \( dW \).  
- Impact des gradients sur chaque poids en fonction de la classe cible.

### ğŸ§· Bonus exploratoire : visualisation des poids
- Reshape de chaque ligne de \( W \) en image 28Ã—28.  
- Visualisation des â€œpatternsâ€ appris par chaque neurone reprÃ©sentant les chiffres typiques.

## âœ… Jour 5 : Optimisation, RÃ©organisation, Affichage 

### ğŸ§ª ExpÃ©rimentations de modÃ¨les
- CrÃ©ation et entraÃ®nement de plusieurs variantes de CNN :
  - [ ] CNN de base sans Dropout ni BatchNorm.
  - [ ] CNN + Dropout uniquement.
  - [ ] CNN + BatchNorm uniquement.
  - [ ] CNN + Dropout + BatchNorm.
- Utilisation de diffÃ©rents optimizers :
  - [ ] SGD
  - [ ] Adam
  - [ ] RMSprop
- Mesure des performances pour chaque combinaison (accuracy max, vitesse de convergence).


### ğŸ“Š Performances observÃ©es

| Architecture                | Optimizer | Acc. max | Convergence |
|-----------------------------|-----------|----------|-------------|
| CNN                         | SGD       | ~98.8%   | ~15 epochs  |
| CNN + Dropout               | SGD       | ~99.1%   | ~10 epochs  |
| CNN + BatchNorm             | SGD       | 99.3%    | 5 epochs    |
| CNN + Dropout + BatchNorm   | SGD       | **99.36%**| **4 epochs**|

### âš™ï¸ Techniques approfondies
- **Batch Normalization** :
  - Ajout de `nn.BatchNorm2d` aprÃ¨s chaque couche convolutionnelle.
  - Normalisation des activations pour chaque batch.
  - AmÃ©liore la stabilitÃ© et accÃ©lÃ¨re la convergence.
- **Dropout** :
  - Ajout de `nn.Dropout(p=0.3)` pour rÃ©gularisation.
  - AppliquÃ© aprÃ¨s ReLU et dans les couches fully connected.
- Test de la combinaison BatchNorm + Dropout :
  - Fonctionne bien si les modules sont bien placÃ©s.


### ğŸ§  ComprÃ©hensions thÃ©oriques
- **BatchNorm** :
  - RÃ©duit l'effet du covariate shift.
  - Rend l'entraÃ®nement moins sensible aux initialisations.
  - Permet des taux d'apprentissage plus Ã©levÃ©s.
- **Dropout** :
  - RÃ©duction du surapprentissage.
  - Fonctionne comme une rÃ©gularisation stochastique.
- **Combinaison** :
  - Dropout + BatchNorm fonctionne bien mais doit Ãªtre positionnÃ© intelligemment.


### ğŸ—ƒï¸ Organisation du projet
- RÃ©organisation du code et des sorties dans une arborescence claire 
- Nettoyage des fichiers temporaires.
- SÃ©paration claire des modules : entraÃ®nement, visualisation, analyse.


### ğŸ“ˆ Visualisation & Analyse
- Sauvegarde automatique des mÃ©triques (`loss`, `accuracy`) dans des fichiers `.pkl`.
- Comparaison visuelle via des courbes matplotlib.
- Export des figures sous forme d'images `.png`.


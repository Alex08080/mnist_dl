ğŸ§  Projet MNIST â€” Semaine 1 : Fondations
Ce dÃ©pÃ´t contient les premiÃ¨res Ã©tapes dâ€™un projet de classification dâ€™images avec PyTorch, basÃ© sur le dataset MNIST. Lâ€™objectif est dâ€™explorer les bases du Deep Learning, de structurer un projet de maniÃ¨re professionnelle, et de suivre sa progression au fil des jours.

âœ… **Jour 1 : Setup et installation**
  
  ğŸ”§ Installation de Git + crÃ©ation du dÃ©pÃ´t local
  
  ğŸ Mise en place dâ€™un environnement virtuel (venv) pour Ã©viter les conflits de dÃ©pendances 
  
  ğŸ“¦ Installation des bibliothÃ¨ques essentielles :
        numpy
        
        matplotlib
        
        torch
        
        torchvision
        
  ğŸ“š Lecture de la documentation officielle pour comprendre les concepts suivants :
  
        PyTorch : Tensors, torch.nn, DataLoader
        
        Torchvision : datasets et transformations
        
        Matplotlib : visualisation de donnÃ©es
  
ğŸ—ƒï¸ **Jour 2 : Chargement des donnÃ©es**

  ğŸ“¥ Dataset chargÃ© via torchvision.datasets.MNIST avec download=True
  
  âš™ï¸ Utilisation du DataLoader pour gÃ©rer les batches et le shuffle
  
  ğŸ” Test du fonctionnement de next(iter(...)) pour inspecter un batch
  
  ğŸ“Š Visualisation dâ€™exemples de chiffres avec plt.subplots() :
  
        gestion des axes
        
        mise en page avec tight_layout()
        
  ğŸ§± DÃ©but de structuration du code :
  
        Encapsulation du chargement dans une classe MNISTDataLoader
        
        SÃ©paration claire des responsabilitÃ©s

  ğŸ›¡ï¸ CrÃ©ation dâ€™un .gitignore adaptÃ© :
  
        fichiers temporaires Python
        
        fichiers liÃ©s Ã  PyTorch
        
        fichiers systÃ¨me Windows
        
        modÃ¨les sauvegardÃ©s (*.pth)
        
ğŸ§  **Jour 3 : Premier classifieur convolutionnel**

  ğŸ§± Construction dâ€™un modÃ¨le NeuralNet :
  
        Convolution â†’ ReLU â†’ MaxPooling
        
        Flatten des tensors
        
        CrossEntropyLoss (intÃ¨gre log softmax)
        
  ğŸ“ ComprÃ©hension du rÃ´le de chaque opÃ©ration :
  
        convolutions pour extraire les features
        
        Flatten pour passer du 3D au 1D
        
  ğŸ” Mise en place de la boucle dâ€™entraÃ®nement :
  
        Parcours par batch
        
        Calcul de la loss moyenne
        
        Backpropagation avec loss.backward()
        
        Mise Ã  jour des poids avec optimizer.step()
        
  ğŸ§ª Ã‰valuation du modÃ¨le :
  
        Passage en mode eval()
        
        DÃ©sactivation du calcul des gradients (with torch.no_grad())
        
        Calcul de lâ€™accuracy via argmax(dim=1) + comparaison avec labels
        
  ğŸ“ˆ RÃ©sultats obtenus :
  
        99.14 % dâ€™accuracy avec 2 couches convolutionnelles (8 epochs)
        
        99.31 % avec 3 couches convolutionnelles (20 epochs)
        
  ğŸ’¾ Sauvegarde du modÃ¨le avec torch.save()
